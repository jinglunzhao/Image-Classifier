{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n\n# mark start time\nstart_time = time.time()\n\n%matplotlib inline\nnp.random.seed(1)\n\n# Loading the dataset\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n\n# Flatten the training and test images\nX_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\nX_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n# Normalize image vectors\nX_train = X_train_flatten/255.\nX_test = X_test_flatten/255.\n# Convert training and test labels to one hot matrices\nY_train = convert_to_one_hot(Y_train_orig, 6)\nY_test  = convert_to_one_hot(Y_test_orig, 6)\n\n# print (\"number of training examples = \" + str(X_train.shape[1]))\n# print (\"number of test examples = \" + str(X_test.shape[1]))\n# print (\"X_train shape: \" + str(X_train.shape))\n# print (\"Y_train shape: \" + str(Y_train.shape))\n# print (\"X_test shape: \" + str(X_test.shape))\n# print (\"Y_test shape: \" + str(Y_test.shape))\n\n# create_placeholders\ndef create_placeholders(n_x, n_y):\n\n    X = tf.placeholder(tf.float32, [n_x, None])\n    Y = tf.placeholder(tf.float32, [n_y, None])\n    return X, Y\n\n# initialize_parameters\ndef initialize_parameters():\n    \n    tf.set_random_seed(1)                   \n    W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [6,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n\n#     weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n#     W1 = tf.get_variable(\"W1\", dtype=tf.float32, shape=[25,12288], initializer=weight_initer)\n#     b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n#     W2 = tf.get_variable(\"W2\", dtype=tf.float32, shape=[12,25], initializer=weight_initer)\n#     b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n#     W3 = tf.get_variable(\"W3\", dtype=tf.float32, shape=[6,12], initializer=weight_initer)\n#     b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n\n    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n    return parameters\n\n# forward_propagation\ndef forward_propagation(X, parameters):\n\n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n\n    Z1 = tf.add(tf.matmul(W1, X), b1)                                            \n    A1 = tf.nn.relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)\n    A2 = tf.nn.relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)\n    \n    return Z3\n\n# compute cost\ndef compute_cost(Z3, Y):\n   \n    # transpose to fit the tensorflow requirement\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    return cost\n\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    X, Y = create_placeholders(n_x, n_y)              # Create Placeholders of shape (n_x, n_y)\n    parameters = initialize_parameters()              # Initialize parameters\n    Z3 = forward_propagation(X, parameters)           # Forward propagation: Build the forward propagation in the tensorflow graph\n    cost = compute_cost(Z3, Y)                        # Cost function: Add cost function to tensorflow graph\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost) # Backpropagation: Define the tensorflow optimizer.\n#     optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n    init = tf.global_variables_initializer()          # Initialize all the variables\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                \n                epoch_cost += minibatch_cost / minibatch_size\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per fives)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print(\" %s seconds \" % (time.time() - start_time))\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        return parameters\n\nparameters = model(X_train, Y_train, X_test, Y_test, num_epochs = 500, minibatch_size = 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From /srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From <ipython-input-6-27b138a5a12b>:91: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nCost after epoch 0: 0.474848\nCost after epoch 100: 0.295894\nCost after epoch 200: 0.187573\nCost after epoch 300: 0.122858\nCost after epoch 400: 0.084200\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}